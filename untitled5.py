# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12O5LFscuAbKGR3cCHwMx67bOQmgrXpst
"""

"""
lstm_transfer_experiment.py

Single-file experiment:
 - Generate synthetic multivariate time series (or load CSV)
 - Baseline persistence / seasonal-naive
 - Build, train, and optimize an LSTM (Keras)
 - Transfer learning: pre-train on large related data, freeze and fine-tune
 - Evaluate with RMSE, MAE, MAPE, WAPE, MASE
 - Save models and predictions

Run: python lstm_transfer_experiment.py
"""

import os
import math
import random
from typing import Tuple, Dict, Any
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks, models, optimizers, losses, metrics

# ---------------------------
# If you want to use your uploaded file, set DATA_PATH to it.
# Developer-provided upload path (use this path to transform to a URL in your tooling if needed):
UPLOADED_FILE_PATH = "/mnt/data/Screenshot 2025-11-21 152703.png"
# If you have a CSV timeseries, set DATA_PATH to that CSV path (or leave None to use synthetic)
DATA_PATH = None
# ---------------------------

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)


# ---------------------------
# Synthetic multivariate series
# ---------------------------
def generate_multivariate_series(
    length: int = 3000,
    n_features: int = 5,
    seasonal_periods=(24, 168),
    trend_scale=0.0008,
    noise_std=0.05,
    structural_breaks=2,
) -> np.ndarray:
    """
    Returns array shape (length, n_features)
    """
    t = np.arange(length)
    data = np.zeros((length, n_features), dtype=np.float32)

    for f in range(n_features):
        phase = np.random.uniform(0, 2 * np.pi)
        amp = np.random.uniform(0.6, 1.6)
        seasonal = np.zeros(length)
        for p in seasonal_periods:
            seasonal += amp * np.sin(2 * np.pi * t / p + phase) / len(seasonal_periods)
        trend = trend_scale * t * (1.0 + 0.3 * np.random.randn())
        noise = np.random.normal(0, noise_std, length)
        data[:, f] = seasonal + trend + noise

    # cross-feature mixing + structural breaks
    for b in range(structural_breaks):
        pos = np.random.randint(length // 10, length - length // 10)
        mag = np.random.uniform(-0.8, 0.8)
        width = np.random.randint(5, 80)
        ramp = np.clip((t - pos + width) / width, 0, 1)
        for f in range(n_features):
            data[:, f] += mag * ramp * (0.5 + 0.5 * np.random.randn())

    # small shared component
    if n_features >= 2:
        for i in range(1, n_features):
            data[:, i] += 0.08 * data[:, 0]

    return data


# ---------------------------
# Windowing utilities
# ---------------------------
def make_windows(series: np.ndarray, input_len: int, pred_len: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    series: (T, features)
    returns X: (N, input_len, features), Y: (N, pred_len, features)
    """
    T, F = series.shape
    indices = range(0, T - input_len - pred_len + 1)
    X = []
    Y = []
    for i in indices:
        X.append(series[i : i + input_len])
        Y.append(series[i + input_len : i + input_len + pred_len])
    return np.stack(X).astype(np.float32), np.stack(Y).astype(np.float32)


# ---------------------------
# Metrics
# ---------------------------
def rmse(pred, true):
    return float(np.sqrt(np.mean((pred - true) ** 2)))


def mae(pred, true):
    return float(np.mean(np.abs(pred - true)))


def mape(pred, true):
    # avoid division by zero
    denom = np.maximum(np.abs(true), 1e-8)
    return float(np.mean(np.abs((pred - true) / denom)) * 100.0)


def wape(pred, true):
    denom = np.sum(np.abs(true))
    return float(np.sum(np.abs(pred - true)) / (denom + 1e-8))


def mase(pred, true, insample, seasonality=1):
    # insample: (T_ins, features)
    # naive error
    denom = np.mean(np.abs(insample[seasonality:] - insample[:-seasonality]))
    mae_val = np.mean(np.abs(pred - true))
    return float(mae_val / (denom + 1e-8))


# ---------------------------
# Baseline methods
# ---------------------------
def persistence_forecast(X: np.ndarray, pred_len: int) -> np.ndarray:
    # use last observed value
    last = X[:, -1:, :]  # (N,1,F)
    return np.repeat(last, pred_len, axis=1)


def seasonal_naive_forecast(X: np.ndarray, pred_len: int, seasonality: int = 24) -> np.ndarray:
    # pick value from t-seasonality (if available) else fallback to last
    N, L, F = X.shape
    out = np.zeros((N, pred_len, F), dtype=np.float32)
    for i in range(N):
        for h in range(pred_len):
            idx = L - seasonality + h
            if 0 <= idx < L:
                out[i, h] = X[i, idx]
            else:
                out[i, h] = X[i, -1]
    return out


# ---------------------------
# LSTM model builder (Keras)
# ---------------------------
def build_lstm_model(
    n_features: int,
    input_len: int,
    pred_len: int,
    units: int = 128,
    n_layers: int = 2,
    dropout: float = 0.2,
    learning_rate: float = 1e-3,
) -> keras.Model:
    inp = layers.Input(shape=(input_len, n_features))
    x = inp
    for i in range(n_layers - 1):
        x = layers.LSTM(units, return_sequences=True)(x)
        if dropout:
            x = layers.Dropout(dropout)(x)
    # last LSTM without return_sequences
    x = layers.LSTM(units)(x)
    x = layers.Dropout(dropout)(x)
    # project to pred_len * n_features
    out = layers.Dense(pred_len * n_features)(x)
    out = layers.Reshape((pred_len, n_features))(out)
    model = keras.Model(inp, out)
    opt = optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=opt, loss="mse")
    return model


# ---------------------------
# Training loops & transfer learning
# ---------------------------
def train_model(
    model: keras.Model,
    X_train,
    Y_train,
    X_val,
    Y_val,
    epochs=50,
    batch_size=64,
    patience=8,
    model_name="model",
):
    cb = [
        callbacks.EarlyStopping(monitor="val_loss", patience=patience, restore_best_weights=True),
        callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=max(2, patience // 3), min_lr=1e-6),
    ]
    history = model.fit(
        X_train,
        Y_train,
        validation_data=(X_val, Y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=cb,
        verbose=2,
    )
    # save model
    model.save(f"{model_name}.keras")
    return history


def transfer_learning_workflow(
    pretrained_model: keras.Model,
    freeze_until_layer_idx: int,
    X_target_train,
    Y_target_train,
    X_target_val,
    Y_target_val,
    fine_tune_epochs=30,
    batch_size=64,
    lr=1e-4,
):
    # clone model to avoid altering original?
    model = keras.models.clone_model(pretrained_model)
    model.set_weights(pretrained_model.get_weights())

    # freeze early layers
    for idx, layer in enumerate(model.layers):
        layer.trainable = False if idx <= freeze_until_layer_idx else True

    # recompile with smaller lr
    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss="mse")
    history = model.fit(
        X_target_train,
        Y_target_train,
        validation_data=(X_target_val, Y_target_val),
        epochs=fine_tune_epochs,
        batch_size=batch_size,
        callbacks=[callbacks.EarlyStopping(monitor="val_loss", patience=6, restore_best_weights=True)],
        verbose=2,
    )
    model.save("fine_tuned.keras")
    return model, history


# ---------------------------
# Experiment runner
# ---------------------------
def run_experiment(
    use_csv=False,
    csv_path=None,
    length=3000,
    n_features=5,
    input_len=96,
    pred_len=24,
    pretrain_length=5000,
    pretrain_features=5,
    grid_params=None,
    seed=SEED,
):
    np.random.seed(seed)
    tf.random.set_seed(seed)

    # 1) Load or generate data
    if use_csv and csv_path is not None:
        # assume csv layout: time,index or time,col1,col2,... ; choose last n_features columns
        df = pd.read_csv(csv_path)
        arr = df.iloc[:, -n_features :].values.astype(np.float32)
        series = arr
    else:
        series = generate_multivariate_series(length=length, n_features=n_features)

    # Train/Val/Test split (chronological)
    T = series.shape[0]
    t_train = int(T * 0.7)
    t_val = int(T * 0.85)
    train_series = series[:t_train]
    val_series = series[t_train - input_len : t_val]  # include window overlap
    test_series = series[t_val - input_len :]

    # scaler fit on train
    scaler = StandardScaler()
    scaler.fit(train_series)
    train_scaled = scaler.transform(train_series)
    val_scaled = scaler.transform(val_series)
    test_scaled = scaler.transform(test_series)

    # windowing
    X_train, Y_train = make_windows(train_scaled, input_len=input_len, pred_len=pred_len)
    X_val, Y_val = make_windows(val_scaled, input_len=input_len, pred_len=pred_len)
    X_test, Y_test = make_windows(test_scaled, input_len=input_len, pred_len=pred_len)

    print(f"Shapes: X_train {X_train.shape}, Y_train {Y_train.shape}, X_val {X_val.shape}, X_test {X_test.shape}")

    # Baselines on test set
    pers_preds = persistence_forecast(X_test, pred_len)
    season_preds = seasonal_naive_forecast(X_test, pred_len, seasonality=24)

    # inverse scale for metrics
    pers_preds_un = scaler.inverse_transform(pers_preds.reshape(-1, n_features)).reshape(pers_preds.shape)
    season_preds_un = scaler.inverse_transform(season_preds.reshape(-1, n_features)).reshape(season_preds.shape)
    Y_test_un = scaler.inverse_transform(Y_test.reshape(-1, n_features)).reshape(Y_test.shape)

    baselines = {
        "persistence": {
            "rmse": rmse(pers_preds_un, Y_test_un),
            "mae": mae(pers_preds_un, Y_test_un),
            "mape": mape(pers_preds_un, Y_test_un),
            "wape": wape(pers_preds_un, Y_test_un),
        },
        "seasonal_naive": {
            "rmse": rmse(season_preds_un, Y_test_un),
            "mae": mae(season_preds_un, Y_test_un),
            "mape": mape(season_preds_un, Y_test_un),
            "wape": wape(season_preds_un, Y_test_un),
        },
    }

    print("Baseline results:", baselines)

    # 2) Hyperparameter grid (simple)
    if grid_params is None:
        grid_params = {
            "units": [64, 128],
            "n_layers": [1, 2],
            "dropout": [0.0, 0.2],
            "lr": [1e-3],
            "batch_size": [64],
            "epochs": [40],
        }

    best_model = None
    best_val_loss = float("inf")
    best_config = None

    # Simple grid search
    for units in grid_params["units"]:
        for n_layers in grid_params["n_layers"]:
            for dropout in grid_params["dropout"]:
                lr = grid_params["lr"][0]
                batch_size = grid_params["batch_size"][0]
                epochs = grid_params["epochs"][0]
                print(f"\nTraining config units={units} layers={n_layers} dropout={dropout}")
                model = build_lstm_model(n_features, input_len, pred_len, units=units, n_layers=n_layers, dropout=dropout, learning_rate=lr)
                h = train_model(model, X_train, Y_train, X_val, Y_val, epochs=epochs, batch_size=batch_size, patience=8, model_name=f"lstm_u{units}_l{n_layers}_d{int(dropout*100)}")
                val_loss = min(h.history["val_loss"])
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model = model
                    best_config = {"units": units, "n_layers": n_layers, "dropout": dropout, "lr": lr, "batch_size": batch_size}

    print("Best config:", best_config, "best_val_loss", best_val_loss)

    # Evaluate best model on test
    preds = best_model.predict(X_test)
    preds_un = scaler.inverse_transform(preds.reshape(-1, n_features)).reshape(preds.shape)
    test_un = scaler.inverse_transform(Y_test.reshape(-1, n_features)).reshape(Y_test.shape)

    metrics_best = {
        "rmse": rmse(preds_un, test_un),
        "mae": mae(preds_un, test_un),
        "mape": mape(preds_un, test_un),
        "wape": wape(preds_un, test_un),
        "mase": mase(preds_un, test_un, train_series, seasonality=1),
    }
    print("Best LSTM test metrics:", metrics_best)

    # 3) Transfer learning: Pre-train on a larger synthetic dataset (related domain), then fine-tune
    print("\nPretraining on larger synthetic dataset for transfer learning...")
    pre_series = generate_multivariate_series(length=pretrain_length, n_features=pretrain_features)
    scaler_pre = StandardScaler()
    scaler_pre.fit(pre_series[: int(0.7 * pretrain_length)])  # fit on pretrain in-sample
    pre_scaled = scaler_pre.transform(pre_series)
    # create windows for pretraining (bigger dataset -> more windows)
    X_pre, Y_pre = make_windows(pre_scaled, input_len=input_len, pred_len=pred_len)
    # split pretrain into train/val
    X_pre_train, X_pre_val, Y_pre_train, Y_pre_val = train_test_split(X_pre, Y_pre, test_size=0.15, shuffle=False)

    # build pretrain model (same architecture as best_model)
    pt_units = best_config["units"]
    pt_layers = best_config["n_layers"]
    pt_dropout = best_config["dropout"]
    pre_model = build_lstm_model(n_features=pretrain_features, input_len=input_len, pred_len=pred_len, units=pt_units, n_layers=pt_layers, dropout=pt_dropout, learning_rate=1e-3)
    print("Pretraining model summary:")
    pre_model.summary()
    train_model(pre_model, X_pre_train, Y_pre_train, X_pre_val, Y_pre_val, epochs=20, batch_size=128, patience=5, model_name="pretrained_lstm")

    # Now transfer: adapt weights from pre_model to best_model architecture when features match.
    # If feature counts differ, a common approach is to copy only layers that don't depend on input size
    # Here we assume same n_features for simplicity. If different, you'd need a custom adapter layer.
    if pretrain_features == n_features:
        # load pre_model weights into a clone structure that matches best_model layers
        # We'll take pre_model weights and apply them to a newly-built model with same architecture and n_features.
        transfer_model = build_lstm_model(n_features, input_len, pred_len, units=pt_units, n_layers=pt_layers, dropout=pt_dropout, learning_rate=1e-4)
        transfer_model.set_weights(pre_model.get_weights())
        # freeze early LSTM layers (freeze first (n_layers-1) LSTM layers)
        freeze_until = 1 + (pt_layers - 1) * 2  # rough index bound; fine control below
        # More robust approach: freeze first half of layers by name
        for i, layer in enumerate(transfer_model.layers):
            if isinstance(layer, layers.LSTM):
                # freeze first LSTM layer
                layer.trainable = False
                break
        # fine-tune on target train set (scaled using scaler fitted on target train)
        print("Fine-tuning transferred model on target data...")
        transfer_model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss="mse")
        transfer_model, history_ft = transfer_learning_workflow(
            pretrained_model=transfer_model,
            freeze_until_layer_idx=0,
            X_target_train=X_train,
            Y_target_train=Y_train,
            X_target_val=X_val,
            Y_target_val=Y_val,
            fine_tune_epochs=30,
            batch_size=best_config["batch_size"],
            lr=1e-4,
        )

        preds_tl = transfer_model.predict(X_test)
        preds_tl_un = scaler.inverse_transform(preds_tl.reshape(-1, n_features)).reshape(preds_tl.shape)
        metrics_tl = {
            "rmse": rmse(preds_tl_un, test_un),
            "mae": mae(preds_tl_un, test_un),
            "mape": mape(preds_tl_un, test_un),
            "wape": wape(preds_tl_un, test_un),
            "mase": mase(preds_tl_un, test_un, train_series, seasonality=1),
        }
        print("Transfer-learned LSTM test metrics:", metrics_tl)
    else:
        print("Pretrain feature dim differs from target; skipping direct weight transfer. Use adapter or shared encoder approach.")

    # Save predictions & metrics
    np.savez_compressed(
        "results_lstm_experiment.npz",
        preds_best=preds_un,
        y_test=test_un,
        persistence=pers_preds_un,
        seasonal=season_preds_un,
    )
    print("Saved predictions to results_lstm_experiment.npz")

    return {
        "baselines": baselines,
        "best_config": best_config,
        "best_metrics": metrics_best,
        "transfer_metrics": (metrics_tl if pretrain_features == n_features else None),
    }


if __name__ == "__main__":
    results = run_experiment(
        use_csv=False,
        csv_path=DATA_PATH,
        length=3000,
        n_features=5,
        input_len=96,
        pred_len=24,
        pretrain_length=5000,
    )
    print("Experiment complete. Summary:")
    print(results)